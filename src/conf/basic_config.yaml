defaults:
  - _self_
  - hydra: basic
  - dataset: basic
  - goal: redundancy
  - override hydra/launcher: joblib

capacity:
  mode: False
  interrupt: False
  threshold: 0.95
  start: 0.2
  increment: 0.2
  stop: 1.2

params:
  exp_name : 'Hopfield'
  neurons: 100
  patterns: 20
  epochs: 1000
  sequential_updates: False
  threshold: None #format 0.99 for 99%
  simple_symmetric: False #symmetric learning without change to gradient
  reps: 5 #check if this changes anything
  num_workers: 0 #what option actually speeds things up?
  pref_gpu: False
  seed: 4
  hebbian_init: False
  hebbian_scale:  0.01

storage:
  save_training_acc: False 
  save_model_params: False #saves info quantities
  save_patterns: False
  min_noise: 0
  max_noise: 0.51 #exclusive
  noise_step: 0.1
  noise_level: 0 #this is for eval during training
  
  post_process: False

optim_params:
   name: 'torch.optim.SGD'
   params:
    lr: 0.1 #0.005 for Adam and 0.1 for SGD both seem to work

binning_params:
  name: 'im_net.prob_estim.BinningAdaptiveSize'
  params:
    n_bins: [60, 60]
    edges: [[-20, 20], [-20,20]]
    use_bincenters: False

layer_params:
  initializer:
    name: 'torch.nn.init.uniform_'
    params:
      a: -1.0 # lower limit
      b: 1.0 # upper limit
  hopfield_layer:
    biases: 
      - False
      - False
    input_sizes: 
      - ${params.neurons}
      - ${params.neurons}
    output_size: ${params.neurons}
    gamma: ??
    activation: 'im_net.activation_functions.SumActivation'
    discrete_output_values: [-1, 1]
    start_symmetric: False
    positive_external: True

datamanager_params:
  cp_dir: 'checkpoints'
  cp_spacing: 'log' #'linear' or 'log'
  cp_number: 0


